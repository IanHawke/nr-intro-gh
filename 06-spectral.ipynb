{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function basis methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many reasons why nonlinear numerical relativity is so expensive. The choice of numerical method is one that we can actually control. This links to the idea of *efficiency*: the computational cost of a method to reach a given accuracy. This is heavily problem dependent, but some general points can be made.\n",
    "\n",
    "So far we have looked at finite difference and finite volume methods which converge *polynomially*, as $\\mathcal{O}(\\Delta x^p)$. As the computational cost scales as $N^4$ and $N \\sim \\Delta x^{-1}$, only high order methods are practical when high accuracy (for phasing error, particularly for next generation detectors) is needed.\n",
    "\n",
    "It is possible to do better. However, the setup cost (in both mental and computational effort) is larger, and the more complex methods have their own problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For finite difference methods we assumed the function $q(x)$ was note at specific points $x_i$, giving $q_i$, and fitted an interpolating function, usually a low order polynomial $g(x)$, through those points. We then approximated the derivative at the grid points using the derivative of $g$.\n",
    "\n",
    "In spectral methods we approximate the function using a function basis, usually using orthogonal functions $\\phi_n(x)$, so that\n",
    "$$\n",
    "q(x) = \\sum_n c_n \\phi_n(x) \\, .\n",
    "$$\n",
    "If we set the basis to be the complex exponentials, $\\phi_n(x) = \\exp(i n x)$, then this is a Fourier series. Other standard choices for the basis would be Chebyshev polynomials or Legendre polynomials.\n",
    "\n",
    "The point of using function basis methods is the expectation that they will converge faster than finite difference methods. We know from Fourier series that, for smooth functions, the approximation error converges exponentially with the number of terms in the series. The hope is that this rapid convergence will carry over when applied to PDEs.\n",
    "\n",
    "At this point the solution is exact (provided that $q$ satisfies any convergence theorem requirements, which often include differentiability). However, it is not useful for numerics, as there will typically be infinitely many terms in the sum. To use this approach we need to truncate the expansion to a finite number of terms $N$, and plug it into the evolution equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burgers equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example we will look at Burgers equation\n",
    "$$\n",
    "\\partial_t q + q \\partial_x q = 0\n",
    "$$\n",
    "on $x \\in [0, 2 \\pi]$ with periodic boundary conditions. We will use the Fourier basis, so that\n",
    "$$\n",
    "q(x, t) = \\sum_{n=-N}^N c_n(t) \\exp(i n x) \\, .\n",
    "$$\n",
    "We can then plug this into the equation. This gives\n",
    "$$\n",
    "\\sum_{n=-N}^N \\left( \\partial_t c_n(t) \\right) \\exp(i n x) + \\sum_{m=-N}^N c_m(t) \\left( \\sum_{k=-N}^N i k c_k(t) \\exp(i (k + m) x) \\right) = 0 \\, .\n",
    "$$\n",
    "Gathering exponential terms we get\n",
    "$$\n",
    "\\sum_{n=-N}^N \\exp(i n x) \\left[ \\partial_t c_n(t) + \\sum_{k+m=n} i k c_m(t) c_k(t) \\right] = 0 \\, .\n",
    "$$\n",
    "Applying the standard orthogonality relations we get the evolution equations for the coefficients,\n",
    "$$\n",
    "\\partial_t c_n(t) + \\sum_{k+m=n} i k c_m(t) c_k(t) = 0 \\, .\n",
    "$$\n",
    "This is $2N+1$ evolution equations for the $2N+1$ complex coefficients $c_n(t)$.\n",
    "\n",
    "Note that this system is only closed if we ignore the fact that the nonlinear product could, in principle, couple wave numbers outside our truncated range. For now we ignore this point; however, correctly dealing with this is important for ensuring the long term accuracy of a spectral method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement and check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Method of Lines, with the time evolution provided by `scipy` to get high accuracy. We will evolve a single sine wave forward in time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import ipympl\n",
    "from scipy.integrate import ode\n",
    "from scipy.optimize import root_scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_data(N):\n",
    "    c = np.zeros(2*N+1, dtype=np.complex64)\n",
    "    c[N-1] = 0.5*1j\n",
    "    c[N+1] = -0.5*1j\n",
    "    return c\n",
    "\n",
    "def get_q(c, x):\n",
    "    N = (len(c)-1)//2\n",
    "    q = np.zeros_like(x)\n",
    "    for i in range(2*N+1):\n",
    "        q += np.real(c[i]*np.exp(1j*(i-N)*x))\n",
    "    return q\n",
    "\n",
    "def spectral(t, c):\n",
    "    N = (len(c)-1)//2\n",
    "    dcdt = np.zeros_like(c)\n",
    "    raise NotImplementedError(\"The spectral RHS needs implementing\")\n",
    "    # I would recommend that the following loop structure is used\n",
    "    # The print statement is left in for debugging\n",
    "    # Note that the loop assumes all mode indexes run -N, ..., N\n",
    "    # But Python index means the c array has indexes 0, ... 2 N + 1\n",
    "    # for n in range(-N, N+1):\n",
    "    #     for k in range(-N, N+1):\n",
    "    #         m = n - k\n",
    "    #         if (abs(m) <= N):\n",
    "    #             # print(t, n, m, k, c[m+N], c[k+N])\n",
    "    #             # Take into account that spectral indexing and\n",
    "    #             # internal Python indexing are offset by N\n",
    "                \n",
    "    return dcdt\n",
    "\n",
    "# This uses scipy do do the method of lines\n",
    "# This should make the time integrator error extremely small\n",
    "def evolve_scipy(N, t_end=0.5, dtfac=0.1):\n",
    "    c = initial_data(N)\n",
    "    \n",
    "    dt = dtfac * 1/len(c)**2\n",
    "    r = ode(spectral).set_integrator('zvode', max_step=dt)\n",
    "    r.set_initial_value(c)\n",
    "    while r.successful() and r.t < t_end:\n",
    "        if r.t + dt > t_end:\n",
    "            dt = t_end - r.t\n",
    "        r.integrate(r.t + dt)\n",
    "    return r.y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also use the method of characteristics to compute the exact solution, which works fine up until characteristic breaking time $t \\simeq 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_root(q, x, t):\n",
    "    return q - np.sin(x - q * t)\n",
    "\n",
    "def exact(xx, t):\n",
    "    q_exact = np.zeros_like(xx)\n",
    "    for i, x in enumerate(xx):\n",
    "        sol = root_scalar(exact_root, args=(x, t), bracket=[-1, 1])\n",
    "        q_exact[i] = sol.root\n",
    "    return q_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does the evolution and plots the result at t=0.5\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "N = 5\n",
    "t_end = 0.5\n",
    "c = evolve_scipy(N, t_end=t_end)\n",
    "plt.figure()\n",
    "plt.plot(x, get_q(c, x), label=rf\"$N={N}$\")\n",
    "plt.plot(x, exact(x, t_end), 'r--', label='exact')\n",
    "plt.legend()\n",
    "plt.title(rf'$t={t_end:.1f}$')\n",
    "plt.xlabel(r'$x$')\n",
    "plt.ylabel(r'$q(x)$')\n",
    "plt.xlim(0, 2*np.pi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the solution is very accurately captured. Now check the convergence rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ns = 2*np.arange(3, 9)\n",
    "errors = np.zeros(len(Ns))\n",
    "for i, N in enumerate(Ns):\n",
    "    c = evolve_scipy(N, t_end=t_end)\n",
    "    q = get_q(c, x)\n",
    "    q_exact = exact(x, t_end)\n",
    "    errors[i] = np.linalg.norm(q - q_exact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(Ns, errors, 'kx', label=\"Spectral errors\")\n",
    "p = np.polyfit(Ns, np.log(errors), 1)\n",
    "plt.semilogy(Ns, np.exp(p[1]+p[0]*Ns), 'b--', label=rf\"$\\propto \\exp({p[0]:.2f}N)$\")\n",
    "plt.legend()\n",
    "plt.xlabel(r'$N$')\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential convergence, as we wanted. This is *so* much more efficient than polynomial scaling methods. As a sketch plot, assume that methods order one, two, and four happen to have the same error when $N=6$ as here. Then we can plot how the errors would go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "Ns = np.linspace(6, 40)\n",
    "plt.semilogy(Ns, np.exp(p[1]+p[0]*Ns), 'b--', label=rf\"$\\propto \\exp({p[0]:.2f}N)$\")\n",
    "plt.semilogy(Ns, np.exp(p[1]+p[0]*Ns[0])*(Ns/Ns[0])**(-1), 'r--', label=r\"$\\propto N^{-1}$\")\n",
    "plt.semilogy(Ns, np.exp(p[1]+p[0]*Ns[0])*(Ns/Ns[0])**(-2), 'g--', label=r\"$\\propto N^{-2}$\")\n",
    "plt.semilogy(Ns, np.exp(p[1]+p[0]*Ns[0])*(Ns/Ns[0])**(-4), 'k--', label=r\"$\\propto N^{-4}$\")\n",
    "plt.legend()\n",
    "plt.xlabel(r'$N$')\n",
    "plt.ylabel(\"Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The huge difference in accuracy is clear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems with spectral methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dealing with nonlinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nonlinear term $q \\partial_x q$ was manipulated into a specific form that couples (in a relatively straightforward way) different Fourier modes. This is not possible in general, although the mode coupling is generic.\n",
    "\n",
    "The standard approach to dealing with this is to use a *pseudo-spectral method*. The truncated basis coefficients are equivalent to representing the function values on a grid. This means that we compute the nonlinear term in real space, and then transform it back to compute the coefficients. For numerical stability this grid is typically *not* uniformly spaced. The cost of transforming between real space and frequency space reduces the efficiency of the method, but does not dominate the cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time stepping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard Courant limit restricts the timestep to be inversely proportional to the grid spacing. When the spectral method is phrased in terms of frequency space there is no grid spacing, so the limiting timestep is unclear. When using a pseudo-spectral method with an uneven grid, the link to the *smallest* grid spacing is clearer, but still not fully transparent. The key point result of more detailed stability calculations is that the timestep goes inversely with the *square* of the number of modes. This significantly reduces the timestep, making higher accuracy much more expensive and increasing accumulated errors from the time integrators.\n",
    "\n",
    "It is still typically true that spectral methods are more efficient than (say) finite difference methods, but the difference is not as large as initial expectations would suggest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discontinuities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fourier series famously exhibit *Gibb's oscillations*: under/over-shoots when representing discontinuities, whose maximum error does not converge as the number of coefficients increases. More generally, any loss of differentiability in the continuum solution makes the spectral representation converge polynomially (just like a finite difference method), with the convergence order linked to the smoothness of the function.\n",
    "\n",
    "Numerical methods will be directly affected whenever the function being approximated is steep enough to appear to lose smoothness. As a lot of matter models, particularly fluids, will naturally steepen gradients through (eg) acoustic modes, this is a real problem.\n",
    "\n",
    "The current consensus is that spectral methods should not be used for matter simulations, or at least not for the matter sector. Some use spectral methods for the spacetime sector and finite volume methods on a separate grid for the matter sector. In principle shocks in the matter sector should lead to a loss of smoothness in the spacetime; however, the impact of this has not been seen. This is a very complex approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parallel efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With finite difference methods, the approximate derivative at a single point depends on a small number of its neighbours. When using parallelisation to speed up the simulation, only these small number of neighbours need to be communicated between compute cores.\n",
    "\n",
    "With a spectral method, the function at any one point depends on *all* the coefficents. Equivalently, in a pseudo-spectral method its derivative depends on the values at *all* the collocation points. This makes parallelisation much more complicated to implement and much less efficient.\n",
    "\n",
    "The usual *hope* is that the huge improvement in the convergence rate is sufficient to reduce the need for parallel computing in the first place. This is typically not sufficient for full NR simulations, however, so some parallelistation is still needed.\n",
    "\n",
    "The standard approach is to use *spectral elements*. The domain is split into a small number of elements and *separate* spectral expansions are used in each element. Then the \"only\" thing that needs communicating is the function values at the element boundaries. This helps significantly, but good choices of elements can involve a lot of trial and error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral methods can be *remarkably* touchy when applied to domains with complex boundaries. In general this is not a problem in NR - asymptotic flatness and few relevant \"interior\" solid boundaries see to this. However, there are a range of cases where features *act like* boundaries: spectral element methods, neutron star surfaces, and in black hole singularity excision. Many of the problems are made worse as these features are typically dynamic, so the boundary moves. \n",
    "\n",
    "The usual solution is to work with spectral elements where each element can be mapped to a simple shape cube or a sphere. Minimizing the distortion of the map helps stability. Again, there is trial and error involved in making this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discontinuous Galerkin methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discontinuous Galerkin (DG) methods are a type of spectral element method. They split the domain into elements (like the cells in finite volume methods), and use a function basis inside each element. The main point is that the values of the function at the element boundaries are not required to match. This allows for discontinuities in the solution (in principle!), and is particularly nice with conservation laws. *In principle* they combine flexibility with efficiency. As each element communicates very little with its neighbours (even less than high order finite difference methods), they are very parallelisable. As the number of basis functions within a single element can be increased, in smooth regions they can achieve spectral convergence. As each element is (quasi-)independent they can have different sizes, effectively giving mesh refinement.\n",
    "\n",
    "The complexity of the setup of a DG method is, however, high. As yet they have neither the efficiency on smooth solutions of spectral methods, nor the robustness of finite differences and volumes, particularly on discontinuous flows. Their popularity in classical CFD contexts may lead to them being the method of the future in NR, however."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a conservation law (for example) of the form\n",
    "$$\n",
    "\\partial_t q + \\partial_x f(q) = 0 \\, .\n",
    "$$\n",
    "Multiply by a *test function* $w(x)$ and integrate over an element $V_i$, getting\n",
    "$$\n",
    "\\int_{V_i} w \\partial_t q + \\int_{V_i} f(q) \\partial_x w = -\\left[ w f(q) \\right]_{V_i} \\, .\n",
    "$$\n",
    "Note that we have integrated by parts to switch the spatial derivative from the flux function to the test function. As we have not assumed the function is continuous at the element boundary (the \"Discontinuous\" part), we pick up a boundary term due to the boundary flux jump, which is moved to the right hand side.\n",
    "\n",
    "Now assume an orthogonal basis $\\phi_n(x)$ for all functions we consider (this is the \"Galerkin\" part) and write\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q = \\sum_n q_n(t) \\phi_n(x) \\, , \\\\\n",
    "w = \\sum_k w_k \\phi_k(x) \\, .\n",
    "\\end{aligned}\n",
    "$$\n",
    "Plugging this into the equation of motion gives\n",
    "$$\n",
    "\\sum_k w_k \\left\\{ \\sum_n \\partial_t q_n \\int_{V_i} \\phi_n \\phi_k + \\sum_n f(q_n) \\int_{V_i} \\phi_n \\partial_x \\phi_k \\right\\} = -\\left[ w f(q) \\right]_{V_i} \\, .\n",
    "$$\n",
    "As the test function is arbitrary we get equations of the form\n",
    "$$\n",
    "M \\partial_t \\mathbf{q} + S^T \\mathbf{f}(\\mathbf{q}) = -\\mathbf{F} \\, ,\n",
    "$$\n",
    "where the *mass matrix* $M$ and *stiffness matrix* $S$ are given by integrals over the element of the basis functions and their derivatives, and the *force* (or *flux*) vector corrects the first and last nodal values to account for the discontinuity at the element boundaries. The mass and stiffness matrices depend only on the choice of basis functions and the size of the element, so can be pre-computed for efficiency.\n",
    "\n",
    "There are two function bases of interest. When considering modes it is typical to use Legendre polynomials $P_n(x)$. When considering nodes it is typical to use *indicator functions* - Lagrange polynomials that are 1 at a single node in the element and zero at all other nodes in the element. That means the coefficients for the basis functions are exactly the function values of the solutions at the nodes. The indicator functions are generally not orthogonal, so calculations requiring orthogonality are done using the modes, and nonlinear calculations (for example, of the flux function) are done using the nodes. The two bases are related by a Vandermonde matrix, which again can be pre-computed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.polynomial import legendre\n",
    "\n",
    "# quadpy may require a licence. It's very useful if you can get one.\n",
    "#import quadpy\n",
    "# If not, use the hack below for the Gauss-Lobatto points.\n",
    "\n",
    "def gl_nodes(m):\n",
    "    nodes = np.zeros(m+1)\n",
    "    nodes[0] = -1\n",
    "    nodes[-1] = 1\n",
    "    # p is the Legendre polynomial P_m\n",
    "    p = np.zeros(m+1)\n",
    "    p[-1] = 1\n",
    "    # The internal Gauss-Lobatto nodes are the roots of P'_{m}\n",
    "    nodes[1:-1] = legendre.legroots(legendre.legder(p))\n",
    "\n",
    "    return nodes\n",
    "\n",
    "def flux(q):\n",
    "    return q**2/2\n",
    "\n",
    "def dg_solve(m, Ne, t_end=0.1):\n",
    "    # If you have a licence for quadpy this is more accurate, especially for large m\n",
    "    # GL = quadpy.c1.gauss_lobatto(m+1)\n",
    "    # nodes = GL.points\n",
    "    nodes = gl_nodes(m)\n",
    "    # To go from modal to nodal we need the Vandermonde matrix\n",
    "    V_hat = legendre.legvander(nodes, m)\n",
    "    c = np.eye(m+1)\n",
    "    # Orthonormalize\n",
    "    for p in range(m+1):\n",
    "        V_hat[:, p] /= np.sqrt(2/(2*p+1))\n",
    "        c[p, p] /= np.sqrt(2/(2*p+1))\n",
    "    d_V_hat = legendre.legval(nodes, legendre.legder(c)).T\n",
    "    # The mass matrix comes from the Vandermonde matrix\n",
    "    M = np.linalg.inv(V_hat @ V_hat.T)\n",
    "    M_inv = V_hat @ V_hat.T\n",
    "    # The stiffness matrix is also linked\n",
    "    K = (M @ (d_V_hat @ np.linalg.inv(V_hat))).T\n",
    "    # The above matrices are on a \"reference\" element.\n",
    "    # The true matrices need scaling to the physical width,\n",
    "    # which might vary element-to-element\n",
    "    Minvs = []\n",
    "    Ks = []\n",
    "    all_nodes = np.zeros((m+1)*(Ne+2))\n",
    "    dx_e = 1/Ne\n",
    "    for e in range(Ne+2):\n",
    "        Minvs.append(M_inv / dx_e * 2 )\n",
    "        Ks.append(K)\n",
    "        all_nodes[e*(m+1):(e+1)*(m+1)] = ((e-1)+(1+nodes)/2) * dx_e\n",
    "    q0 = np.sin(2*np.pi*all_nodes)\n",
    "\n",
    "    # A slight hack: define the RHS nested inside,\n",
    "    # to give access to the mode number m without explicitly passing it through.\n",
    "    # This means the RHS function can be used directly with scipy\n",
    "    def rhs(t, q):\n",
    "        dqdt_nodal = np.zeros_like(q)\n",
    "        dqdt = np.zeros_like(q)\n",
    "        # Loop over elements\n",
    "        for e in range(1, Ne+1):\n",
    "            lo = e*(m+1)\n",
    "            hi = (e+1)*(m+1)\n",
    "            dqdt_nodal[lo:hi] += Ks[e] @ flux(q[lo:hi])\n",
    "            # LF, alpha = 1\n",
    "            dqdt_nodal[lo] += 0.5 * (flux(q[lo]) + flux(q[lo-1]) - (q[lo] - q[lo-1]))\n",
    "            dqdt_nodal[hi-1] -= 0.5 * (flux(q[hi]) + flux(q[hi-1]) - (q[hi] - q[hi-1]))\n",
    "            dqdt[lo:hi] = Minvs[e] @ dqdt_nodal[lo:hi]\n",
    "        # Periodic boundaries\n",
    "        dqdt[:m+1] = dqdt[-2*(m+1):-(m+1)]\n",
    "        dqdt[-(m+1):] = dqdt[(m+1):2*(m+1)]\n",
    "        return dqdt\n",
    "\n",
    "    cfl = 0.1/(2*m+1)\n",
    "    # Again use scipy with high order integration\n",
    "    r = ode(rhs).set_integrator('dopri5', max_step=cfl*dx_e)\n",
    "    r.set_initial_value(q0)\n",
    "    while r.successful() and r.t < t_end:\n",
    "        if r.t + cfl*dx_e > t_end:\n",
    "            r.integrate(t_end)\n",
    "        else:\n",
    "            r.integrate(r.t + cfl*dx_e)\n",
    "\n",
    "    return all_nodes, r.y\n",
    "\n",
    "fig, ax = plt.subplots(3, 1, figsize=(5, 11))\n",
    "\n",
    "m = 4 # modes\n",
    "Ne = 8 # Number of elements\n",
    "t_end = 0.1\n",
    "all_nodes, soln = dg_solve(m, Ne, t_end)\n",
    "q0 = np.sin(2*np.pi*all_nodes)\n",
    "\n",
    "ax[0].plot(all_nodes, q0, 'k--', label=\"Initial\")\n",
    "ax[0].plot(all_nodes, soln, 'b-x', label=rf\"t={t_end:.1f}\")\n",
    "# Note the differing domains lead to the scaling here.\n",
    "ax[0].plot(all_nodes, exact(2*np.pi*all_nodes, 2*np.pi*t_end), 'r--', label=rf\"Exact t={t_end:.1f}\")\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(r\"$x$\")\n",
    "ax[0].set_ylabel(rf\"$q, m={m}, N={Ne}$\")\n",
    "ax[0].set_xlim(0, 1);\n",
    "\n",
    "Ne = 20\n",
    "ms = np.arange(2, 7)\n",
    "errors = np.zeros(len(ms))\n",
    "for i, m in enumerate(ms):\n",
    "    all_nodes, soln = dg_solve(m, Ne, t_end)\n",
    "    errors[i] = np.linalg.norm(soln - exact(2*np.pi*all_nodes, 2*np.pi*t_end)) # This drops the dx factor!\n",
    "ax[1].semilogy(ms, errors, 'kx', label='Errors')\n",
    "pp = np.polyfit(ms, np.log(errors), 1)\n",
    "ax[1].semilogy(ms, np.exp(pp[1]+pp[0]*ms), 'b--', label=rf\"$\\propto \\exp({pp[0]:.2f} m)$\")\n",
    "ax[1].set_xlabel(r\"Modes $m$\")\n",
    "ax[1].set_ylabel(r\"Error\")\n",
    "ax[1].set_xticks(ms)\n",
    "ax[1].set_xticklabels(ms)\n",
    "ax[1].legend()\n",
    "\n",
    "m = 4\n",
    "Nes = 2**np.arange(4, 9)\n",
    "errors3 = np.zeros(len(Nes))\n",
    "errors4 = np.zeros(len(Nes))\n",
    "for i, Ne in enumerate(Nes):\n",
    "    all_nodes3, soln3 = dg_solve(3, Ne, t_end)\n",
    "    all_nodes4, soln4 = dg_solve(4, Ne, t_end)\n",
    "    errors3[i] = np.linalg.norm(soln3 - exact(2*np.pi*all_nodes3, 2*np.pi*t_end))/np.sqrt(Ne)\n",
    "    errors4[i] = np.linalg.norm(soln4 - exact(2*np.pi*all_nodes4, 2*np.pi*t_end))/np.sqrt(Ne)\n",
    "ax[2].loglog(Nes, errors3, 'kx', label='Errors m=3')\n",
    "ax[2].loglog(Nes, errors4, 'r+', label='Errors m=4')\n",
    "pp3 = np.polyfit(np.log(Nes), np.log(errors3), 1)\n",
    "pp4 = np.polyfit(np.log(Nes), np.log(errors4), 1)\n",
    "ax[2].loglog(Nes, np.exp(pp3[1]+pp3[0]*np.log(Nes)), 'k--', label=rf\"$\\propto N^{{{pp3[0]:.2f}}}$\")\n",
    "ax[2].loglog(Nes, np.exp(pp4[1]+pp4[0]*np.log(Nes)), 'r--', label=rf\"$\\propto N^{{{pp4[0]:.2f}}}$\")\n",
    "ax[2].set_xlabel(r\"Modes $m$\")\n",
    "ax[2].set_ylabel(r\"Error\")\n",
    "ax[2].set_xticks(Nes)\n",
    "ax[2].set_xticklabels(Nes)\n",
    "ax[2].legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the uneven nature of the grid, which is natural for DG methods. The first simulation effectively uses 32 bits of information (8 elements, 4 modes/nodes per element) but still captures the solution well.\n",
    "\n",
    "We see the expected result: by modifying the number of modes we get spectral convergence, and by modifying the number of elements we get polynomial convergence linked to the number of modes used."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
